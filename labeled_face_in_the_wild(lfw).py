# -*- coding: utf-8 -*-
"""Labeled_Face_in_the_wild(LFW).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tMWcrAN8CwTUefbigIgYNqD3tUW3N5P_

The Labeled Faces in the Wild (LFW) dataset is a public benchmark for face verification, consisting of 13,000 labeled images of faces from the web. Each face has a name label, and there are 1,680 unique individuals in the dataset. It is widely used to develop and test face recognition systems.

**Loading Data directly from kaggle**
"""

! pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

! mkdir ~/.kaggle

! cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download atulanandjha/lfwpeople

ls -l

! unzip lfwpeople.zip

import tarfile

# Extract lfw-funneled.tgz
with tarfile.open('lfw-funneled.tgz', 'r:gz') as tar:
    tar.extractall()

# Verify extraction
!ls lfw_funneled

"""Load And Preprocess The Data"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Path to the extracted images
data_path = 'lfw_funneled'

# Initialize lists to hold images and labels
images = []
labels = []

# Load images and labels
for person_name in os.listdir(data_path):
    person_folder = os.path.join(data_path, person_name)
    if os.path.isdir(person_folder):
        for image_name in os.listdir(person_folder):
            image_path = os.path.join(person_folder, image_name)
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            image = cv2.resize(image, (47, 62))  # Resize all images to a common size
            images.append(image)
            labels.append(person_name)

# Convert lists to numpy arrays
images = np.array(images).astype('float32') / 255.0
images = np.expand_dims(images, axis=-1)  # Add channel dimension
labels = np.array(labels)

# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)
labels_categorical = to_categorical(labels_encoded)

# Split dataset into training, validation, and test sets
train_images, test_images, train_labels, test_labels = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)

# Print dataset shapes
print("Train images shape:", train_images.shape)
print("Validation images shape:", val_images.shape)
print("Test images shape:", test_images.shape)
print("Train labels shape:", train_labels.shape)
print("Validation labels shape:", val_labels.shape)
print("Test labels shape:", test_labels.shape)

"""**ANN**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

# Define the ANN model
ann_model = Sequential([
    Flatten(input_shape=(62, 47, 1)),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(len(np.unique(labels_encoded)), activation='softmax')
])

# Compile the model
ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history_ann = ann_model.fit(train_images, train_labels, epochs=20, batch_size=64, validation_data=(val_images, val_labels))

# Evaluate the model
ann_loss, ann_accuracy = ann_model.evaluate(test_images, test_labels)
print(f"ANN Test Accuracy: {ann_accuracy}")

"""**CNN**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Define the CNN model
cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(62, 47, 1)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(len(np.unique(labels_encoded)), activation='softmax')
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history_cnn = cnn_model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_data=(val_images, val_labels))

# Evaluate the model
cnn_loss, cnn_accuracy = cnn_model.evaluate(test_images, test_labels)
print(f"CNN Test Accuracy: {cnn_accuracy}")

import matplotlib.pyplot as plt

# Plot training & validation accuracy and loss for ANN
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_ann.history['accuracy'], label='Train Accuracy')
plt.plot(history_ann.history['val_accuracy'], label='Val Accuracy')
plt.title('ANN Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_ann.history['loss'], label='Train Loss')
plt.plot(history_ann.history['val_loss'], label='Val Loss')
plt.title('ANN Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Plot training & validation accuracy and loss for CNN
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')
plt.title('CNN Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history_cnn.history['loss'], label='Train Loss')
plt.plot(history_cnn.history['val_loss'], label='Val Loss')
plt.title('CNN Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()